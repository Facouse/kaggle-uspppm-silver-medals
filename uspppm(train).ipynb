{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModel, AdamW, AutoTokenizer\n",
    "import sys\n",
    "import scipy\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "import scipy.stats\n",
    "\n",
    "class CFG:\n",
    "    result_dir = '/home/xuming/workspace/pppm' # result dir\n",
    "    data_dir = '/home/xuming/workspace/us-patent-phrase-to-phrase-matching' # data dir\n",
    "    k_folds = 5 # k folds\n",
    "    n_jobs = 5 # n_jobs\n",
    "    seed = 42 # random seed\n",
    "    device = torch.cuda.is_available() # use cuda\n",
    "    print_freq = 100 # print frequency\n",
    "    \n",
    "    model_name = 'bert-for-patents' # model name  # electra-large / deberta-v3-large / funnel-large / bert-for-patents\n",
    "    base_epoch = 5 # epoch\n",
    "    batch_size = 32 # batch size\n",
    "    lr = 1e-5 # learning rate\n",
    "    seq_length = 200 # sequence length\n",
    "    max_grad_norm = 1 # gradient clipping\n",
    "    \n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "\n",
    "\n",
    "class KFold(object):\n",
    "    \"\"\"\n",
    "    Group split by group_col\n",
    "    \"\"\"\n",
    "    def __init__(self, k_folds=10, flag_name='fold_flag'):\n",
    "        self.k_folds = k_folds # k folds\n",
    "        self.flag_name = flag_name # fold_flag\n",
    "\n",
    "    def group_split(self, train_df, group_col): \n",
    "        group_value = list(set(train_df[group_col])) # group value\n",
    "        group_value.sort() # sort\n",
    "        fold_flag = [i % self.k_folds for i in range(len(group_value))] # fold_flag\n",
    "        np.random.shuffle(fold_flag) # shuffle\n",
    "        train_df = train_df.merge(pd.DataFrame({group_col: group_value, self.flag_name: fold_flag}), how='left', on=group_col) # merge\n",
    "        return train_df\n",
    "\n",
    "def get_data():\n",
    "    train_df = pd.read_csv(CFG.data_dir + '/train.csv') # train data\n",
    "    train_df = KFold(CFG.k_folds).group_split(train_df, group_col='anchor') # kfold group split\n",
    "    titles = get_cpc_texts() # cpc texts\n",
    "    train_df = get_text(train_df, titles) # # train data get text\n",
    "    test_df = pd.read_csv(CFG.data_dir + '/test.csv') # test data\n",
    "    test_df['score'], test_df['fold_flag'] = 0, -1 # test fill score and fold_flag\n",
    "    test_df = get_text(test_df, titles) # # test data get text\n",
    "    print(train_df.shape, test_df.shape) # print shape\n",
    "    return train_df, test_df # return train and test data\n",
    "\n",
    "def get_text(df, titles):\n",
    "    df['anchor'] = df['anchor'].apply(lambda x:x.lower()) # anchor lower\n",
    "    df['target'] = df['target'].apply(lambda x:x.lower()) # target lower\n",
    "    # title\n",
    "    df['title'] = df['context'].map(titles)\n",
    "    df['title'] = df['title'].apply(lambda x:x.lower().replace(';', '').replace('  ',' ').strip())\n",
    "\n",
    "    df = df.join(df.groupby(['anchor', 'context']).target.agg(list).rename('gp_targets'), on=['anchor', 'context']) # group by anchor and context and get target_list\n",
    "    df['gp_targets'] = df.apply(lambda x: ', '.join([i for i in x['gp_targets'] if i != x['target']]), axis=1) # get gp_targets\n",
    "    df['text'] = df['anchor'] + '[SEP]' + df['target'] + '[SEP]'  + df['title'] + '[SEP]'  + df['gp_targets'] # anchor [SEP] target [SEP] title [SEP] gp_targets\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_cpc_texts():\n",
    "    '''\n",
    "    get cpc texts\n",
    "    '''\n",
    "    # get cpc codes\n",
    "    contexts = []  \n",
    "    pattern = '[A-Z]\\d+'\n",
    "    for file_name in os.listdir(f'{CFG.data_dir}/cpc-data/CPCSchemeXML202105'):\n",
    "        result = re.findall(pattern, file_name)\n",
    "        if result:\n",
    "            contexts.append(result)\n",
    "    contexts = sorted(set(sum(contexts, []))) # all unique cpc codes\n",
    "    # like ['A01', 'A21', 'A22', 'A23', 'A24', 'A41', 'A42', 'A43', 'A44', 'A45']\n",
    "    \n",
    "    results = {}\n",
    "    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n",
    "        with open(f'{CFG.data_dir}/cpc-data/CPCTitleList202202/cpc-section-{cpc}_20220201.txt') as f:\n",
    "            s = f.read()\n",
    "        # 总目录及其text 如 \"A\t\tHUMAN NECESSITIES\"\n",
    "        pattern = f'{cpc}\\t\\t.+' \n",
    "        result = re.findall(pattern, s)\n",
    "        pattern = \"^\"+pattern[:-2]\n",
    "        cpc_result = re.sub(pattern, \"\", result[0]) # 获取描述，如 'HUMAN NECESSITIES'\n",
    "\n",
    "        for context in [c for c in contexts if c[0] == cpc]:\n",
    "            pattern = f'{context}\\t\\t.+'\n",
    "            result = re.findall(pattern, s) # cpc code及其text 如 'A01\\t\\tAGRICULTURE; FORESTRY; ANIMAL HUSBANDRY; HUNTING; TRAPPING; FISHING'\n",
    "            pattern = \"^\"+pattern[:-2]\n",
    "            results[context] = cpc_result + \". \" + re.sub(pattern, \"\", result[0]) # 生成字典 like {'A01': 'HUMAN NECESSITIES. AGRICULTURE; FORESTRY; ANIMAL HUSBANDRY; HUNTING; TRAPPING; FISHING'}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatentDataset(Dataset):\n",
    "    def __init__(self, meta_data: pd.DataFrame, tokenizer, fold: int = -1, mode='train'):\n",
    "        self.meta_data = meta_data.copy() # meta_data\n",
    "        self.meta_data.reset_index(drop=True, inplace=True) # reset index\n",
    "        if mode == 'train':\n",
    "            self.meta_data = self.meta_data[self.meta_data['fold_flag'] != fold].copy() # train data\n",
    "        elif mode == 'valid':\n",
    "            self.meta_data = self.meta_data[self.meta_data['fold_flag'] == fold].copy() # valid data\n",
    "        elif mode == 'test':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(mode)\n",
    "        self.meta_data.reset_index(drop=True, inplace=True) # reset index\n",
    "        self.seq_length = CFG.seq_length # seq_length\n",
    "        if tokenizer.sep_token != '[SEP]': \n",
    "            self.meta_data['text'] = self.meta_data['text'].apply(lambda x:x.replace('[SEP]', tokenizer.sep_token )) # replace [SEP] to tokenizer.sep_token\n",
    "        self.text = self.meta_data['text'].values # text\n",
    "        self.target = self.meta_data['score'].values # target\n",
    "        self.mode = mode\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = self.text[index] # seq\n",
    "        target = self.target[index] # target\n",
    "        encoded = self.tokenizer.encode_plus(\n",
    "            text=seq, # text\n",
    "            add_special_tokens=True, # add_special_tokens \n",
    "            max_length=self.seq_length, # max_length\n",
    "            padding='max_length', # padding\n",
    "            return_attention_mask=True, # return_attention_mask\n",
    "            return_tensors='pt', # return_tensors\n",
    "            truncation=True # truncation\n",
    "        )\n",
    "        input_ids = encoded['input_ids'][0] # input_ids\n",
    "        attention_mask = encoded['attention_mask'][0] # attention_mask\n",
    "        \n",
    "        # input_ids: torch.Size([32, 200]) # padding 为 0\n",
    "        # like tensor([[    2, 20211,  3269,  ...,     0,     0,     0],\n",
    "        #              [    2,  2785,  9669,  ...,     0,     0,     0]], device='cuda:0')\n",
    "\n",
    "        # attention_mask: torch.Size([32, 200]) # padding 为 0，其余为1\n",
    "        # like tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
    "        #              [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
    "\n",
    "        # target: torch.Size([32])\n",
    "        # like tensor([0.2500, 0.5000, 0.5000, 0.5000, 0.5000, 0.2500, 0.5000, 0.2500, 0.0000,\n",
    "        #              0.0000, 0.5000, 0.0000, 0.5000, 0.5000, 0.2500, 0.2500, 0.5000, 0.5000,\n",
    "        #              0.0000, 0.0000, 0.2500, 0.5000, 0.0000, 0.0000, 0.5000, 0.2500, 0.7500,\n",
    "        #              0.2500, 0.2500, 1.0000, 0.7500, 0.5000], device='cuda:0')\n",
    "\n",
    "        return input_ids, attention_mask, np.array(target, dtype=np.float32) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta_data) # len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatentModel(nn.Module):\n",
    "    def __init__(self, name, num_classes=1, pretrained=True):\n",
    "        super(PatentModel, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(name) # config\n",
    "        self.attention_probs_dropout_prob=0. # attention_probs_dropout_prob\n",
    "        self.hidden_dropout_prob=0. # hidden_dropout_prob\n",
    "        if pretrained:\n",
    "            self.encoder = AutoModel.from_pretrained(name, config=self.config) \n",
    "        else:\n",
    "            self.encoder = AutoModel.from_config(self.config)\n",
    "        in_dim = self.encoder.config.hidden_size # get hidden_size\n",
    "        self.last_fc = nn.Linear(in_dim, num_classes) # last_fc\n",
    "        torch.nn.init.normal_(self.last_fc.weight, std=0.02) # init last_fc\n",
    "        self.sig = nn.Sigmoid() # Sigmoid\n",
    "\n",
    "    def forward(self, seq, seq_mask):\n",
    "        x = self.encoder(seq, attention_mask=seq_mask)[\"last_hidden_state\"] # forward                       # torch.Size([32, 200, 1024])\n",
    "        x = torch.sum(x * seq_mask.unsqueeze(-1), dim=1) / torch.sum(seq_mask, dim=1).unsqueeze(-1) # mean  # torch.Size([32, 1024])\n",
    "        out = self.last_fc(x) # last_fc                                                                     # torch.Size([32, 1])\n",
    "        out = self.sig(out) # Sigmoid                                                                       # torch.Size([32, 1])\n",
    "        out = torch.squeeze(out)                                                                            # torch.Size([32])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_test_df(df, tokenizer, batch_size):\n",
    "    # input ids lengths list \n",
    "    input_lengths = [] \n",
    "    for text in df['text'].fillna(\"\").values:\n",
    "        length = len(tokenizer(text, add_special_tokens=True)['input_ids'])\n",
    "        input_lengths.append(length)\n",
    "    df['input_lengths'] = input_lengths\n",
    "    length_sorted_idx = np.argsort([-l for l in input_lengths])\n",
    "\n",
    "    # sort dataframe by lengths\n",
    "    sort_df = df.iloc[length_sorted_idx]\n",
    "    # calc max_len per batch\n",
    "    sorted_input_length = sort_df['input_lengths'].values # \n",
    "    batch_max_length = np.zeros_like(sorted_input_length) # zeros_like \n",
    "    # every batch\n",
    "    for i in range((len(sorted_input_length)//batch_size)+1):\n",
    "        batch_max_length[i*batch_size:(i+1)*batch_size] = np.max(sorted_input_length[i*batch_size:(i+1)*batch_size]) # max input length in every batch\n",
    "    sort_df['batch_max_length'] = batch_max_length\n",
    "    return sort_df, length_sorted_idx\n",
    "    \n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset() # reset\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0. \n",
    "        self.avg = 0.\n",
    "        self.sum = 0.\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def save_model(model, save_path, model_name):\n",
    "    '''\n",
    "    save model\n",
    "    '''\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    filename = os.path.join(save_path, model_name + '.pth.tar')\n",
    "    torch.save({'state_dict': model.state_dict(), }, filename)\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"\n",
    "    Handles PyTorch x Numpy seeding issues.\n",
    "\n",
    "    Args:\n",
    "        worker_id (int): Id of the worker.\n",
    "    \"\"\"\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "\n",
    "class MSELoss(nn.Module):\n",
    "    '''\n",
    "    MSELoss \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        loss = (inputs - targets) ** 2\n",
    "        loss = loss.mean()\n",
    "        loss = torch.sqrt(loss)\n",
    "        return loss\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    return scipy.stats.pearsonr(y_true, y_pred)[0] # pearsonr\n",
    "\n",
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon=1., emb_name='emb'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name and param.grad is not None:\n",
    "                # print(name, param)\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = epsilon * param.grad / max(norm, 0.001)\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, emb_name='emb'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name and param.grad is not None:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "def get_model_path(model_name):\n",
    "    '''\n",
    "    get model path\n",
    "    '''\n",
    "    res = CFG.result_dir\n",
    "    if model_name in ['electra-base', 'electra-large']:\n",
    "        res += '/electra/' + model_name.split('-')[1] + '-discriminator'\n",
    "    elif model_name == 'deberta-v3-large':\n",
    "        res += '/deberta-v3-large/'\n",
    "    elif model_name == 'funnel-large':\n",
    "        res += '/funnel-large/'\n",
    "    elif model_name == 'bert-for-patents':\n",
    "        res += '/bert-for-patents/'\n",
    "    else:\n",
    "        raise ValueError(model_name)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, scheduler, optimizer, epoch, is_adversial=False):\n",
    "    # training\n",
    "    batch_time = AverageMeter() # batch time\n",
    "    losses = AverageMeter() # loss\n",
    "    # switch to train mode\n",
    "    model.train() # train mode\n",
    "    fgm = FGM(model) if is_adversial else None # fgm\n",
    "    start = time.time()\n",
    "    for i, batch_data in enumerate(train_loader):\n",
    "        # optimizer.zero_grad()\n",
    "        if CFG.device:\n",
    "            batch_data = (t.cuda() for t in batch_data)\n",
    "        seq, seq_mask, target = batch_data\n",
    "        # print(seq.shape,seq_mask.shape,target.shape)\n",
    "        output = model(seq, seq_mask)\n",
    "        # print(seq.shape,seq_mask.shape,target.shape,output.shape)\n",
    "        loss = criterion(output, target) # loss\n",
    "        losses.update(loss.item())\n",
    "        loss = loss\n",
    "        loss.backward()\n",
    "        if is_adversial:\n",
    "            # 对抗训练\n",
    "            fgm.attack()  # 在embedding上添加对抗扰动\n",
    "            output = model(seq, seq_mask) # 模型输出\n",
    "            loss_adv = criterion(output, target)  # 计算loss\n",
    "            loss_adv.backward()  # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n",
    "            fgm.restore()  # 恢复embedding参数\n",
    "        if CFG.max_grad_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm) # 梯度裁剪\n",
    "        optimizer.step() # 更新参数\n",
    "        optimizer.zero_grad() # 清空梯度\n",
    "\n",
    "        batch_time.update(time.time() - start) # update batch time\n",
    "        start = time.time() # update start time\n",
    "        if i % CFG.print_freq == 0: \n",
    "            print('Epoch: [{0}][{1}/{2}], Loss {loss:.4f}\\n'.format(epoch, i, len(train_loader), loss=loss.item())) # print info\n",
    "    return losses.avg, batch_time.sum \n",
    "\n",
    "\n",
    "def validate(model, valid_loader, tokenizer):\n",
    "    model.eval() # eval mode\n",
    "    y_pred = [] # y_pred\n",
    "    for i, batch_data in enumerate(valid_loader):\n",
    "        if CFG.device:\n",
    "            batch_data = (t.cuda() for t in batch_data) # cuda\n",
    "        seq, seq_mask, target = batch_data # batch_data\n",
    "        output = model(seq, seq_mask) # model output\n",
    "        y_pred.append(output.detach().cpu().numpy()) # y_pred\n",
    "    y_pred = np.concatenate(y_pred) # y_pred\n",
    "    score = get_score(valid_loader.dataset.target, y_pred) # score\n",
    "    # scoring\n",
    "    return y_pred, score # y_pred, score\n",
    "\n",
    "\n",
    "print('------------------------------------------------------Training-------------------------------------------------\\n')\n",
    "warnings.filterwarnings('ignore') # ignore warnings\n",
    "model_dir = os.path.join(CFG.result_dir, 'models') # model dir\n",
    "os.makedirs(model_dir, exist_ok=True) # make dir\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print('>> data_processing...\\n')\n",
    "\n",
    "# load data\n",
    "train_df, test_df = get_data() \n",
    "\n",
    "oof_prediction = np.zeros((len(train_df))) # oof prediction\n",
    "eval_loss = [] # eval loss\n",
    "\n",
    "for fold in range(CFG.k_folds):\n",
    "    model = PatentModel(get_model_path(CFG.model_name), pretrained=True) # load model\n",
    "    model.zero_grad() # zero grad\n",
    "    model = model.cuda() # cuda\n",
    "    tokenizer = AutoTokenizer.from_pretrained(get_model_path(CFG.model_name)) # load tokenizer\n",
    "    train_model_filename = os.path.join(model_dir, CFG.model_name + '_fold{}.pth.tar'.format(fold)) # train model filename\n",
    "\n",
    "    if fold == 0:\n",
    "        col_lengths = [] # col lengths\n",
    "        for text in train_df['text'].fillna(\"\").values: # get col lengths\n",
    "            length = len(tokenizer(text, add_special_tokens=True)['input_ids'])\n",
    "            col_lengths.append(length)\n",
    "        print(f'text max(lengths): {max(col_lengths)} {np.percentile(col_lengths, 95)}')\n",
    "\n",
    "    train_dataset = PatentDataset(train_df, tokenizer, fold, mode='train') # train dataset\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=CFG.batch_size, num_workers=CFG.n_jobs, pin_memory=True, worker_init_fn=worker_init_fn) # train loader\n",
    "    valid_dataset = PatentDataset(train_df, tokenizer, fold, mode='valid') # valid dataset \n",
    "    valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=CFG.batch_size * 4, num_workers=CFG.n_jobs, pin_memory=True) # valid loader\n",
    "    criterion = MSELoss() # criterion\n",
    "\n",
    "    best_score = -1\n",
    "    patience_cnt = 0\n",
    "    is_improved = True\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG.lr, betas=(0.9, 0.999), eps=1e-6, weight_decay=0) # optimizer\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.base_epoch, eta_min=CFG.lr / 5) # scheduler\n",
    "    for epoch in range(CFG.base_epoch):\n",
    "        scheduler.step(epoch=epoch) # scheduler\n",
    "        print('Fold: [{0}] Epoch: [{1}], lr:[{2}]\\n'.format(fold, epoch, optimizer.param_groups[0]['lr'])) # print Fold, Epoch, lr\n",
    "        is_adversial = True # 对抗训练\n",
    "        train_loss, train_batch_time = train(model, train_loader, criterion, scheduler, optimizer, epoch, is_adversial) # train\n",
    "        print('Epoch avg loss: {0}, Epoch cost time:{1} min\\n'.format(train_loss, train_batch_time / 60)) # print Epoch avg loss, Epoch cost time\n",
    "        with torch.no_grad(): \n",
    "            y_pred, score = validate(model, valid_loader, tokenizer) # validate\n",
    "            print('Epoch score: {0}\\n'.format(score)) # print Epoch score\n",
    "            if score > best_score:\n",
    "                best_score, best_epoch = score, epoch # best_score, best_epoch\n",
    "                oof_prediction[np.where(train_df['fold_flag'] == fold)] = y_pred.copy() # oof prediction\n",
    "                save_model(model, model_dir, '{}_fold{}_seed{}'.format(CFG.model_name, fold, CFG.seed)) # save model\n",
    "                print('********Best Epoch: [{0}], Best Score:{1}********\\n'.format(best_epoch, best_score)) # print Best Epoch, Best Score\n",
    "            else:\n",
    "                is_improved = False # is_improved\n",
    "                patience_cnt += 1 # patience_cnt\n",
    "\n",
    "    eval_loss.append(best_score) # eval loss\n",
    "    del model, y_pred\n",
    "    _ = gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "print('CV mean:{} std:{}.'.format(np.mean(eval_loss), np.std(eval_loss))) # CV mean, std\n",
    "print('detail:{}'.format(np.round(eval_loss, 4))) # detail\n",
    "np.save(os.path.join(CFG.result_dir, CFG.model_name + '_oof.npy'), oof_prediction) # save oof prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b79a61544c9a744d09395b396d14bdc3ab2980641b64ddb1c7bc6d7b892900a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
